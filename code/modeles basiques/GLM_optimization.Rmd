---
title: "GLM optimization"
author: "AKROUT Leyth, BOULAHFA Jawad, DE SANTIAGO Kylliann"
date: "22 février 2021"
output:
  prettydoc::html_pretty:
    toc: true
    theme: cayman
    highlight: github
    df_print: paged
---

```{r, echo=FALSE}
rm(list = ls())
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)
library(caret)
library(MASS)
library(doParallel) # calcul parallèle
library(glmnet)
# source("~/suivi-du-data-camp-DeSantiago_Boulahfa_Akrout/code/projet_fct.R")
source("C:/Users/Startklar/suivi-du-data-camp-DeSantiago_Boulahfa_Akrout/code/projet_fct.R")
# source("./projet_fct.R")
```

# Chargement et réorganisation des données

Chargement du jeu de données.
```{r}
# data = read.csv("~/suivi-du-data-camp-DeSantiago_Boulahfa_Akrout/code/data_train.csv")
data = read.csv("data_train.csv")
head(data)
data=data[,-1]
head(data)
```

On change les types des variables qui sont mal codées.
```{r}
data=data %>% mutate(sequence = as.factor(sequence)) %>% 
  mutate(seq_be = as.factor(seq_be)) %>% 
  mutate(seq_af = as.factor(seq_af)) %>% 
  mutate(sequence = as.factor(sequence)) %>% 
  mutate(structure = as.factor(structure)) %>%
  mutate(struct_be = as.factor(struct_be)) %>% 
  mutate(struct_af = as.factor(struct_af)) %>% 
  mutate(predicted_loop_type = as.factor(predicted_loop_type)) %>% 
  mutate(loop_type_be = as.factor(loop_type_be)) %>% 
  mutate(loop_type_af = as.factor(loop_type_af)) 
```


On filtre maintenant les données qui ont SN_filter=0.
```{r}
print(sum(is.na(data)))
print(sum(data$SN_filter==0)/68) # Nombre d'individus dont le SN_filter=0
data=data[data$SN_filter==1,]
rownames(data)=NULL#1:108052
```

Mise à 0 des valeurs négatives.
```{r}
#p=dim(data)[2]
#sum(data[,(p-4):p]<0) # nb de lignes où il existe des labels qui ont des valeurs négatives
#for (i in (p-4):p)
#{
#  data[which(data[,i]<0),i]=0
#}
#sum(data[,(p-4):p]<0)
```

Plutôt que mettre à 0 les valeurs négatives, on va effectuer une translation sur chaque label en soustrayant chacune de ses valeurs par la valeur minimale du label puis en ajoutant un bruit afin de se placer sur $\mathbb{R}_+^*$.
```{r}
p = dim(data)[2]
bruit = 1e-12
```

Initialement, il existe des valeurs négatives pour nos labels.
```{r}
sum(data[,(p-4):p] <= 0) # nb total de lignes où il existe des labels qui ont des valeurs négatives
```

On peut aussi afficher le nombre de lignes où il y a des valeurs négatives pour chacun de nos labels. On peut remarquer que chaque label en possède.
```{r}
sum(data$reactivity <= 0)
sum(data$deg_50C <= 0)
sum(data$deg_Mg_50C <= 0)
sum(data$deg_pH10 <= 0)
sum(data$deg_Mg_pH10 <= 0)
```

On effectue une translation pour se placer sur $\mathbb{R}_+^*$.
```{r}
for (i in (p-4):p)
{
  data[,i] <- data[,i] - min(data[,i]) + bruit # l'ajout du bruit permet d'éliminer les 0.
}
```

On a bien retiré toutes les valeurs négatives.
```{r}
sum(data[,(p-4):p] <= 0) # Il n'y a plus aucune valeur négative
```

# Création de la base train/validation

```{r}
tamp=train_test_split(data)
data_train=tamp$train
data_val=tamp$val
```

```{r}
sum(is.na(data_train))
sum(is.na(data_val))
dim(data_train)[1]+dim(data_val)[1]-dim(data)[1]
```


```{r}
#rm(tamp,data)
```

# Test sur les lois

```{r}
M=mean(data_train$reactivity,na.rm = TRUE)
S=var(data_train$reactivity,na.rm = TRUE)
print(M)
print(S)
```

```{r}
ks.test(data_train$reactivity,"pexp",1/M)
```

Problème ici.
```{r}
alpha_LoiGamma=(M^2)/(S-M^2)
beta_LoiGamma=(S-M^2)/M
ks.test(data_train$reactivity,"pgamma",alpha_LoiGamma,beta_LoiGamma)
```


# Modèles simples : GLM : loi Gamma et inverse.gaussian

Initialisation de vecteurs pour stocker nos résultats.

```{r}
models_reactivity <- c()
errors_reactivity_train <- c()
errors_reactivity_val <- c()
```

```{r}
models_deg_Mg_pH10 <- c()
errors_deg_Mg_pH10_train <- c()
errors_deg_Mg_pH10_val <- c()
```

```{r}
models_deg_pH10 <- c()
errors_deg_pH10_train <- c()
errors_deg_pH10_val <- c()
```

```{r}
models_deg_Mg_50C <- c()
errors_deg_Mg_50C_train <- c()
errors_deg_Mg_50C_val <- c()
```

```{r}
models_deg_50C <- c()
errors_deg_50C_train <- c()
errors_deg_50C_val <- c()
```

## Reactivity

### Loi Gamma

Ici on fera énormément de tests pour essayer de voir quels variables apportent des informations utiles.
```{r}
model = glm(reactivity~sequence, family = Gamma, data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
model=glm(reactivity~(sequence+seq_be+seq_af),family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
model=glm(reactivity~sequence+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```


```{r}
model=glm(reactivity~sequence+predicted_loop_type+index_sequence,family = Gamma,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
model=glm(reactivity~sequence+structure+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
                         index_sequence + structure,
                       family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + seq_be + seq_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure +
              predicted_loop_type,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure +
              predicted_loop_type + loop_type_be,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure +
              predicted_loop_type + loop_type_af,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```


```{r}
model = glm(reactivity  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure +
              predicted_loop_type + loop_type_be + loop_type_af,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

#### Optimisation

```{r}
formula_gamma_reactivity =
  "reactivity  ~ sequence + seq_be + seq_af + index_sequence + structure"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_gamma_reactivity = glm(formula = formula_gamma_reactivity,
                             family = Gamma, data = data_train)
```

```{r}
summary(model_gamma_reactivity)
```

```{r}
y_pred_gamma_train = predict.glm(model_gamma_reactivity,data_train,type="response")
error_gamma_train <- mean((y_pred_gamma_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_gamma_train)

y_pred_gamma_val=predict.glm(model_gamma_reactivity,data_val,type="response")
error_gamma_val <- mean((y_pred_gamma_val-data_val$reactivity)^2,na.rm=TRUE)
print(error_gamma_val)
```

```{r}
models_reactivity <- c(models_reactivity, "model_gamma")
errors_reactivity_train <- c(errors_reactivity_train, error_gamma_train)
errors_reactivity_val <- c(errors_reactivity_val, error_gamma_val)
rm(error_gamma_val, error_gamma_train, y_pred_gamma_train, y_pred_gamma_val)
```

##### StepAIC

```{r}
model_gamma_reactivity_aic = stepAIC(model_gamma_reactivity, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_gamma_reactivity_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_gamma_train_aic = predict.glm(model_gamma_reactivity_aic,data_train,type="response")
mean((y_pred_gamma_train_aic-data_train$reactivity)^2,na.rm=TRUE)

y_pred_gamma_val_aic=predict.glm(model_gamma_reactivity_aic,data_val,type="response")
mean((y_pred_gamma_val_aic-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
rm(model_gamma_reactivity_aic, y_pred_gamma_train_aic, y_pred_gamma_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_gamma_reactivity sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_gamma_reactivity)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_gamma_reactivity_val = glm(formula = formula_gamma_reactivity,
                                 family = Gamma, data = data_val)
data_val_pen = model.matrix(model_gamma_reactivity_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_gamma_reactivity_val)
```

On pénalise le model_gamma_reactivity avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_reactivity_pen_ridge = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_gamma_reactivity_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_reactivity_pen_ridge, "model_gamma_reactivity_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_gamma_reactivity_pen_ridge <- readRDS("model_gamma_reactivity_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_gamma_train = predict(model_gamma_reactivity_pen_ridge, data_train_pen,
                                   type = "response", s = "lambda.min")
y_pred_ridge_gamma_val = predict(model_gamma_reactivity_pen_ridge, data_val_pen,
                                 type = "response", s = "lambda.min")
error_ridge_gamma_train <- mean((y_pred_ridge_gamma_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_ridge_gamma_train)
error_ridge_gamma_val <- mean((y_pred_ridge_gamma_val-data_val$reactivity)^2,na.rm=TRUE)
print(error_ridge_gamma_val)
```

On pénalise le model_gamma_reactivity avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_reactivity_pen_lasso = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_gamma_reactivity_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_reactivity_pen_lasso, "model_gamma_reactivity_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_gamma_reactivity_pen_lasso <- readRDS("model_gamma_reactivity_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_gamma_train = predict(model_gamma_reactivity_pen_lasso, data_train_pen,
                                   type="response", s="lambda.min")
error_lasso_gamma_train <- mean((y_pred_lasso_gamma_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_lasso_gamma_train)

y_pred_lasso_gamma_val = predict(model_gamma_reactivity_pen_lasso, data_val_pen,
                                 type="response", s="lambda.min")
error_lasso_gamma_val <- mean((y_pred_lasso_gamma_val-data_val$reactivity)^2,na.rm=TRUE)
print(error_lasso_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_reactivity <- c(models_reactivity, "model_gamma_ridge", "model_gamma_lasso")
errors_reactivity_train <- c(errors_reactivity_train, error_ridge_gamma_train,
                             error_lasso_gamma_train)
errors_reactivity_val <- c(errors_reactivity_val, error_ridge_gamma_val, error_lasso_gamma_val)
rm(error_ridge_gamma_val, error_lasso_gamma_val,
   error_ridge_gamma_train, error_lasso_gamma_train,
   y_pred_ridge_gamma_train, y_pred_ridge_gamma_val,
   y_pred_lasso_gamma_train, y_pred_lasso_gamma_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_gamma_reactivity =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$reactivity,
#                  y_true_val = data_val$reactivity,
#                  family = Gamma, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_gamma_reactivity)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_gamma_reactivity_pen_enet = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_gamma_reactivity)
#coef(model_gamma_reactivity_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_reactivity_pen_enet, "model_gamma_reactivity_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_gamma_reactivity_pen_enet <- readRDS("model_gamma_reactivity_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_gamma_train = predict(model_gamma_reactivity_pen_enet, data_train_pen,
                                   type = "response", s = "lambda.min")
error_enet_gamma_train <- mean((y_pred_enet_gamma_train - data_train$reactivity)^2, na.rm=TRUE)
print(error_enet_gamma_train)

y_pred_enet_gamma_val = predict(model_gamma_reactivity_pen_enet, data_val_pen,
                                 type = "response", s = "lambda.min")
error_enet_gamma_val <- mean((y_pred_enet_gamma_val - data_val$reactivity)^2, na.rm=TRUE)
print(error_enet_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_reactivity <- c(models_reactivity, "model_gamma_enet")
errors_reactivity_train <- c(errors_reactivity_train, error_enet_gamma_train)
errors_reactivity_val <- c(errors_reactivity_val, error_enet_gamma_val)
rm(error_enet_gamma_train, error_enet_gamma_val,
   y_pred_enet_gamma_train, y_pred_enet_gamma_val)
```

```{r}
data.frame(models_reactivity, errors_reactivity_train, errors_reactivity_val)
```












### Inverse.gaussian

```{r}
model=glm(reactivity~sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
model=glm(reactivity~(sequence+seq_be+seq_af),family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
model=glm(reactivity~sequence+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```


```{r}
model=glm(reactivity~sequence+predicted_loop_type,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```



```{r}
model=glm(reactivity~sequence+structure,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

On stocke le meilleur modèle pour l'inverse gaussian.
```{r}
model=glm(reactivity~sequence+predicted_loop_type,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#                         index_sequence + structure,
#                       family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
# Ne marche pas
#model = glm(reactivity  ~ sequence + seq_be + seq_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

model = glm(reactivity  ~ sequence + seq_be + struct_af +
                         index_sequence + structure,
            family = inverse.gaussian, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(reactivity  ~ sequence + seq_be + struct_be +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + seq_af + struct_be +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + seq_af + struct_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + seq_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + seq_be + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + struct_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)

#model = glm(reactivity  ~ sequence + struct_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
```

```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_be,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_af,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```


```{r}
#model = glm(reactivity  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_be + loop_type_af,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

#### Optimisation

```{r}
formula_inverse_gaussian_reactivity =
  "reactivity  ~ sequence + seq_be + struct_af + index_sequence + structure"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_inverse_gaussian_reactivity = glm(formula = formula_inverse_gaussian_reactivity,
                                        family = inverse.gaussian, data = data_train)
```

```{r}
summary(model_inverse_gaussian_reactivity)
```

```{r}
y_pred_inverse_gaussian_train = predict.glm(model_inverse_gaussian_reactivity,
                                            data_train, type = "response")
error_inverse_gaussian_train <-
  mean((y_pred_inverse_gaussian_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_inverse_gaussian_train)

y_pred_inverse_gaussian_val = predict.glm(model_inverse_gaussian_reactivity,
                                          data_val, type = "response")
error_inverse_gaussian_val <-
  mean((y_pred_inverse_gaussian_val - data_val$reactivity)^2,na.rm=TRUE)
print(error_inverse_gaussian_val)
```

```{r}
models_reactivity <- c(models_reactivity, "model_inverse_gaussian")
errors_reactivity_train <- c(errors_reactivity_train, error_inverse_gaussian_train)
errors_reactivity_val <- c(errors_reactivity_val, error_inverse_gaussian_val)
rm(error_inverse_gaussian_val, error_inverse_gaussian_train,
   y_pred_inverse_gaussian_train, y_pred_inverse_gaussian_val)
```

##### StepAIC

```{r}
model_inverse_gaussian_reactivity_aic =
  stepAIC(model_inverse_gaussian_reactivity, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_inverse_gaussian_reactivity_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_inverse_gaussian_train_aic = predict.glm(model_inverse_gaussian_reactivity_aic,data_train,type="response")
mean((y_pred_inverse_gaussian_train_aic-data_train$reactivity)^2,na.rm=TRUE)

y_pred_inverse_gaussian_val_aic=predict.glm(model_inverse_gaussian_reactivity_aic,data_val,type="response")
mean((y_pred_inverse_gaussian_val_aic-data_val$reactivity)^2,na.rm=TRUE)
```

```{r}
rm(model_inverse_gaussian_reactivity_aic, y_pred_inverse_gaussian_train_aic, y_pred_inverse_gaussian_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_inverse_gaussian_reactivity sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_inverse_gaussian_reactivity)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_inverse_gaussian_reactivity_val = glm(formula = formula_inverse_gaussian_reactivity,
                                 family = inverse.gaussian, data = data_val)
data_val_pen = model.matrix(model_inverse_gaussian_reactivity_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_inverse_gaussian_reactivity_val)
```

On pénalise le model_inverse_gaussian_reactivity avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_reactivity_pen_ridge = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_inverse_gaussian_reactivity_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_reactivity_pen_ridge,
#        "model_inverse_gaussian_reactivity_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_reactivity_pen_ridge <- 
  readRDS("model_inverse_gaussian_reactivity_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_inverse_gaussian_train = predict(model_inverse_gaussian_reactivity_pen_ridge,
                                              data_train_pen,
                                              type = "response", s = "lambda.min")
y_pred_ridge_inverse_gaussian_val = predict(model_inverse_gaussian_reactivity_pen_ridge,
                                            data_val_pen,
                                            type = "response", s = "lambda.min")

error_ridge_inverse_gaussian_train <-
  mean((y_pred_ridge_inverse_gaussian_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_train)

error_ridge_inverse_gaussian_val <-
  mean((y_pred_ridge_inverse_gaussian_val-data_val$reactivity)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_val)
```

On pénalise le model_inverse_gaussian_reactivity avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_reactivity_pen_lasso = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_inverse_gaussian_reactivity_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_reactivity_pen_lasso,
#        "model_inverse_gaussian_reactivity_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_reactivity_pen_lasso <-
  readRDS("model_inverse_gaussian_reactivity_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_inverse_gaussian_train = predict(model_inverse_gaussian_reactivity_pen_lasso,
                                              data_train_pen,
                                              type="response", s="lambda.min")
error_lasso_inverse_gaussian_train <-
  mean((y_pred_lasso_inverse_gaussian_train-data_train$reactivity)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_train)

y_pred_lasso_inverse_gaussian_val = predict(model_inverse_gaussian_reactivity_pen_lasso,
                                            data_val_pen,
                                            type="response", s="lambda.min")
error_lasso_inverse_gaussian_val <-
  mean((y_pred_lasso_inverse_gaussian_val-data_val$reactivity)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_reactivity <- c(models_reactivity, "model_inverse_gaussian_ridge", "model_inverse_gaussian_lasso")
errors_reactivity_train <- c(errors_reactivity_train, error_ridge_inverse_gaussian_train,
                             error_lasso_inverse_gaussian_train)
errors_reactivity_val <- c(errors_reactivity_val, error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val)
rm(error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val,
   error_ridge_inverse_gaussian_train, error_lasso_inverse_gaussian_train,
   y_pred_ridge_inverse_gaussian_train, y_pred_ridge_inverse_gaussian_val,
   y_pred_lasso_inverse_gaussian_train, y_pred_lasso_inverse_gaussian_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_inverse_gaussian_reactivity =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$reactivity,
#                  y_true_val = data_val$reactivity,
#                  family = inverse.gaussian, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_inverse_gaussian_reactivity)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_inverse_gaussian_reactivity_pen_enet = cv.glmnet(data_train_pen, data_train$reactivity,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_inverse_gaussian_reactivity)
#coef(model_inverse_gaussian_reactivity_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_reactivity_pen_enet,
#        "model_inverse_gaussian_reactivity_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_reactivity_pen_enet <-
  readRDS("model_inverse_gaussian_reactivity_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_inverse_gaussian_train = predict(model_inverse_gaussian_reactivity_pen_enet,
                                             data_train_pen,
                                             type = "response", s = "lambda.min")
error_enet_inverse_gaussian_train <-
  mean((y_pred_enet_inverse_gaussian_train - data_train$reactivity)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_train)

y_pred_enet_inverse_gaussian_val = predict(model_inverse_gaussian_reactivity_pen_enet,
                                           data_val_pen,
                                           type = "response", s = "lambda.min")
error_enet_inverse_gaussian_val <-
  mean((y_pred_enet_inverse_gaussian_val - data_val$reactivity)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_reactivity <- c(models_reactivity, "model_inverse_gaussian_enet")
errors_reactivity_train <- c(errors_reactivity_train, error_enet_inverse_gaussian_train)
errors_reactivity_val <- c(errors_reactivity_val, error_enet_inverse_gaussian_val)
rm(error_enet_inverse_gaussian_train, error_enet_inverse_gaussian_val,
   y_pred_enet_inverse_gaussian_train, y_pred_enet_inverse_gaussian_val)
```


### Récapitulatif des résultats

```{r}
models_reactivity_errors_df <- data.frame(models_reactivity,
                                          errors_reactivity_train,
                                          errors_reactivity_val)
```

```{r}
models_reactivity_errors_df
```

Le meilleur modèle (au sens de la minimisation du MSE sur la validation) est le `model_gamma`.
Ainsi, la pénalisation n'a pas permis d'améliorer les performances ici.
```{r}
models_reactivity_errors_df[which.min(errors_reactivity_val), ]
```





## deg_Mg_pH10

### Loi Gamma

Ici on fera énormément de tests pour essayer de voir quels variables apportent des informations utiles.

```{r}
model=glm(deg_Mg_pH10~sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_pH10~(sequence+seq_be+seq_af),family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_pH10~sequence+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_Mg_pH10~sequence+predicted_loop_type+index_sequence,family = Gamma,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```



```{r}
model=glm(deg_Mg_pH10~sequence+structure+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

```

AUTRES ESSAIS
```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
              index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_be + seq_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_be + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_af + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_af + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_be + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = Gamma, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
              seq_be + seq_af + index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type,
#            family = Gamma, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_be,
#            family = Gamma, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_af,
#            family = Gamma, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
#              seq_be + seq_af + index_sequence + structure +
#              predicted_loop_type + loop_type_be + loop_type_af,
#            family = Gamma, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$reactivity)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$reactivity)^2,na.rm=TRUE)
```

#### Optimisation

```{r}
formula_gamma_deg_Mg_pH10 =
  "deg_Mg_pH10  ~ sequence + seq_be + seq_af + index_sequence + structure"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_gamma_deg_Mg_pH10 = glm(formula = formula_gamma_deg_Mg_pH10,
                             family = Gamma, data = data_train)
```

```{r}
summary(model_gamma_deg_Mg_pH10)
```

```{r}
y_pred_gamma_train = predict.glm(model_gamma_deg_Mg_pH10,data_train,type="response")
error_gamma_train <- mean((y_pred_gamma_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_gamma_train)

y_pred_gamma_val=predict.glm(model_gamma_deg_Mg_pH10,data_val,type="response")
error_gamma_val <- mean((y_pred_gamma_val-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_gamma_val)
```

```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_gamma")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_gamma_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_gamma_val)
rm(error_gamma_val, error_gamma_train, y_pred_gamma_train, y_pred_gamma_val)
```

##### StepAIC

```{r}
model_gamma_deg_Mg_pH10_aic = stepAIC(model_gamma_deg_Mg_pH10, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_gamma_deg_Mg_pH10_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_gamma_train_aic = predict.glm(model_gamma_deg_Mg_pH10_aic,data_train,type="response")
mean((y_pred_gamma_train_aic-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred_gamma_val_aic=predict.glm(model_gamma_deg_Mg_pH10_aic,data_val,type="response")
mean((y_pred_gamma_val_aic-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
rm(model_gamma_deg_Mg_pH10_aic, y_pred_gamma_train_aic, y_pred_gamma_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_gamma_deg_Mg_pH10 sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_gamma_deg_Mg_pH10)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_gamma_deg_Mg_pH10_val = glm(formula = formula_gamma_deg_Mg_pH10,
                                 family = Gamma, data = data_val)
data_val_pen = model.matrix(model_gamma_deg_Mg_pH10_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_gamma_deg_Mg_pH10_val)
```

On pénalise le model_gamma_deg_Mg_pH10 avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_Mg_pH10_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_gamma_deg_Mg_pH10_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_pH10_pen_ridge, "model_gamma_deg_Mg_pH10_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_pH10_pen_ridge <- readRDS("model_gamma_deg_Mg_pH10_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_gamma_train = predict(model_gamma_deg_Mg_pH10_pen_ridge, data_train_pen,
                                   type = "response", s = "lambda.min")
y_pred_ridge_gamma_val = predict(model_gamma_deg_Mg_pH10_pen_ridge, data_val_pen,
                                 type = "response", s = "lambda.min")
error_ridge_gamma_train <- mean((y_pred_ridge_gamma_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_ridge_gamma_train)
error_ridge_gamma_val <- mean((y_pred_ridge_gamma_val-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_ridge_gamma_val)
```

On pénalise le model_gamma_deg_Mg_pH10 avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_Mg_pH10_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_gamma_deg_Mg_pH10_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_pH10_pen_lasso, "model_gamma_deg_Mg_pH10_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_pH10_pen_lasso <- readRDS("model_gamma_deg_Mg_pH10_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_gamma_train = predict(model_gamma_deg_Mg_pH10_pen_lasso, data_train_pen,
                                   type="response", s="lambda.min")
error_lasso_gamma_train <- mean((y_pred_lasso_gamma_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_lasso_gamma_train)

y_pred_lasso_gamma_val = predict(model_gamma_deg_Mg_pH10_pen_lasso, data_val_pen,
                                 type="response", s="lambda.min")
error_lasso_gamma_val <- mean((y_pred_lasso_gamma_val-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_lasso_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_gamma_ridge", "model_gamma_lasso")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_ridge_gamma_train,
                             error_lasso_gamma_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_ridge_gamma_val, error_lasso_gamma_val)
rm(error_ridge_gamma_val, error_lasso_gamma_val,
   error_ridge_gamma_train, error_lasso_gamma_train,
   y_pred_ridge_gamma_train, y_pred_ridge_gamma_val,
   y_pred_lasso_gamma_train, y_pred_lasso_gamma_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_gamma_deg_Mg_pH10 =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_Mg_pH10,
#                  y_true_val = data_val$deg_Mg_pH10,
#                  family = Gamma, cores = detectCores() - 2)
```

Au final, le meilleur modèle est celui avec $\alpha = 0.05$.
```{r}
#print(alpha_opti_enet_gamma_deg_Mg_pH10)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_gamma_deg_Mg_pH10_pen_enet = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_gamma_deg_Mg_pH10)
#coef(model_gamma_deg_Mg_pH10_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_pH10_pen_enet, "model_gamma_deg_Mg_pH10_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_pH10_pen_enet <- readRDS("model_gamma_deg_Mg_pH10_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_gamma_train = predict(model_gamma_deg_Mg_pH10_pen_enet, data_train_pen,
                                   type = "response", s = "lambda.min")
error_enet_gamma_train <- mean((y_pred_enet_gamma_train - data_train$deg_Mg_pH10)^2, na.rm=TRUE)
print(error_enet_gamma_train)

y_pred_enet_gamma_val = predict(model_gamma_deg_Mg_pH10_pen_enet, data_val_pen,
                                 type = "response", s = "lambda.min")
error_enet_gamma_val <- mean((y_pred_enet_gamma_val - data_val$deg_Mg_pH10)^2, na.rm=TRUE)
print(error_enet_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_gamma_enet")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_enet_gamma_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_enet_gamma_val)
rm(error_enet_gamma_train, error_enet_gamma_val,
   y_pred_enet_gamma_train, y_pred_enet_gamma_val)
```

```{r}
data.frame(models_deg_Mg_pH10, errors_deg_Mg_pH10_train, errors_deg_Mg_pH10_val)
```














### Inverse.gaussian

```{r}
model=glm(deg_Mg_pH10~sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_pH10~(sequence+seq_be),family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_pH10~sequence+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_Mg_pH10~sequence+predicted_loop_type,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_Mg_pH10~sequence+structure,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


AUTRES ESSAIS
```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_be + struct_af +
              index_sequence + structure,
            family = inverse.gaussian, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_be + seq_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_be + struct_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_pH10  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = inverse.gaussian, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_af + struct_be +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_af + struct_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + seq_be + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_Mg_pH10  ~ sequence + struct_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


#### Optimisation

```{r}
# Cette formule => problème pour extraire matrice de design de la validation
#formula_inverse_gaussian_deg_Mg_pH10 =
#  "deg_Mg_pH10  ~ sequence + seq_be + struct_af + index_sequence + structure"

formula_inverse_gaussian_deg_Mg_pH10 =
  "deg_Mg_pH10  ~ sequence + struct_be + struct_af + index_sequence + structure"
```

On stocke le meilleur modèle pour la loi inverse.gaussian.
```{r}
model_inverse_gaussian_deg_Mg_pH10 = glm(formula = formula_inverse_gaussian_deg_Mg_pH10,
                                        family = inverse.gaussian, data = data_train)
```

```{r}
summary(model_inverse_gaussian_deg_Mg_pH10)
```

```{r}
y_pred_inverse_gaussian_train = predict.glm(model_inverse_gaussian_deg_Mg_pH10,
                                            data_train, type = "response")
error_inverse_gaussian_train <-
  mean((y_pred_inverse_gaussian_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_inverse_gaussian_train)

y_pred_inverse_gaussian_val = predict.glm(model_inverse_gaussian_deg_Mg_pH10,
                                          data_val, type = "response")
error_inverse_gaussian_val <-
  mean((y_pred_inverse_gaussian_val - data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_inverse_gaussian_val)
```

```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_inverse_gaussian")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_inverse_gaussian_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_inverse_gaussian_val)
rm(error_inverse_gaussian_val, error_inverse_gaussian_train,
   y_pred_inverse_gaussian_train, y_pred_inverse_gaussian_val)
```

##### StepAIC

```{r}
model_inverse_gaussian_deg_Mg_pH10_aic =
  stepAIC(model_inverse_gaussian_deg_Mg_pH10, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_inverse_gaussian_deg_Mg_pH10_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_inverse_gaussian_train_aic = predict.glm(model_inverse_gaussian_deg_Mg_pH10_aic,data_train,type="response")
mean((y_pred_inverse_gaussian_train_aic-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred_inverse_gaussian_val_aic=predict.glm(model_inverse_gaussian_deg_Mg_pH10_aic,data_val,type="response")
mean((y_pred_inverse_gaussian_val_aic-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
rm(model_inverse_gaussian_deg_Mg_pH10_aic, y_pred_inverse_gaussian_train_aic,
   y_pred_inverse_gaussian_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_inverse_gaussian_deg_Mg_pH10 sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_inverse_gaussian_deg_Mg_pH10)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_inverse_gaussian_deg_Mg_pH10_val = glm(formula = formula_inverse_gaussian_deg_Mg_pH10,
                                             family = inverse.gaussian, data = data_val)
data_val_pen = model.matrix(model_inverse_gaussian_deg_Mg_pH10_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_inverse_gaussian_deg_Mg_pH10_val)
```

On pénalise le model_inverse_gaussian_deg_Mg_pH10 avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_Mg_pH10_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_inverse_gaussian_deg_Mg_pH10_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_pH10_pen_ridge,
#        "model_inverse_gaussian_deg_Mg_pH10_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_pH10_pen_ridge <- 
  readRDS("model_inverse_gaussian_deg_Mg_pH10_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_pH10_pen_ridge,
                                              data_train_pen,
                                              type = "response", s = "lambda.min")
y_pred_ridge_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_pH10_pen_ridge,
                                            data_val_pen,
                                            type = "response", s = "lambda.min")

error_ridge_inverse_gaussian_train <-
  mean((y_pred_ridge_inverse_gaussian_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_train)

error_ridge_inverse_gaussian_val <-
  mean((y_pred_ridge_inverse_gaussian_val-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_val)
```

On pénalise le model_inverse_gaussian_deg_Mg_pH10 avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_Mg_pH10_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_inverse_gaussian_deg_Mg_pH10_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_pH10_pen_lasso,
#        "model_inverse_gaussian_deg_Mg_pH10_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_pH10_pen_lasso <-
  readRDS("model_inverse_gaussian_deg_Mg_pH10_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_pH10_pen_lasso,
                                              data_train_pen,
                                              type="response", s="lambda.min")
error_lasso_inverse_gaussian_train <-
  mean((y_pred_lasso_inverse_gaussian_train-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_train)

y_pred_lasso_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_pH10_pen_lasso,
                                            data_val_pen,
                                            type="response", s="lambda.min")
error_lasso_inverse_gaussian_val <-
  mean((y_pred_lasso_inverse_gaussian_val-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_inverse_gaussian_ridge", "model_inverse_gaussian_lasso")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_ridge_inverse_gaussian_train,
                             error_lasso_inverse_gaussian_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val)
rm(error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val,
   error_ridge_inverse_gaussian_train, error_lasso_inverse_gaussian_train,
   y_pred_ridge_inverse_gaussian_train, y_pred_ridge_inverse_gaussian_val,
   y_pred_lasso_inverse_gaussian_train, y_pred_lasso_inverse_gaussian_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_inverse_gaussian_deg_Mg_pH10 =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_Mg_pH10,
#                  y_true_val = data_val$deg_Mg_pH10,
#                  family = inverse.gaussian, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_inverse_gaussian_deg_Mg_pH10)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_inverse_gaussian_deg_Mg_pH10_pen_enet = cv.glmnet(data_train_pen, data_train$deg_Mg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_inverse_gaussian_deg_Mg_pH10)
#coef(model_inverse_gaussian_deg_Mg_pH10_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_pH10_pen_enet,
#        "model_inverse_gaussian_deg_Mg_pH10_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_pH10_pen_enet <-
  readRDS("model_inverse_gaussian_deg_Mg_pH10_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_pH10_pen_enet,
                                             data_train_pen,
                                             type = "response", s = "lambda.min")
error_enet_inverse_gaussian_train <-
  mean((y_pred_enet_inverse_gaussian_train - data_train$deg_Mg_pH10)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_train)

y_pred_enet_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_pH10_pen_enet,
                                           data_val_pen,
                                           type = "response", s = "lambda.min")
error_enet_inverse_gaussian_val <-
  mean((y_pred_enet_inverse_gaussian_val - data_val$deg_Mg_pH10)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_pH10 <- c(models_deg_Mg_pH10, "model_inverse_gaussian_enet")
errors_deg_Mg_pH10_train <- c(errors_deg_Mg_pH10_train, error_enet_inverse_gaussian_train)
errors_deg_Mg_pH10_val <- c(errors_deg_Mg_pH10_val, error_enet_inverse_gaussian_val)
rm(error_enet_inverse_gaussian_train, error_enet_inverse_gaussian_val,
   y_pred_enet_inverse_gaussian_train, y_pred_enet_inverse_gaussian_val)
```


### Récapitulatif des résultats

```{r}
models_deg_Mg_pH10_errors_df <- data.frame(models_deg_Mg_pH10,
                                          errors_deg_Mg_pH10_train,
                                          errors_deg_Mg_pH10_val)
```

```{r}
models_deg_Mg_pH10_errors_df
```

Le meilleur modèle (au sens de la minimisation du MSE sur la validation) est le `model_gamma_enet`.
Ainsi, la pénalisation a permis d'améliorer légèrement les performances ici.
```{r}
models_deg_Mg_pH10_errors_df[which.min(errors_deg_Mg_pH10_val), ]
```






## deg_pH10

### Loi Gamma

Ici on fera énormément de tests pour essayer de voir quels variables apportent des informations utiles.

```{r}
model=glm(deg_pH10~sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_pH10~(sequence+seq_be+seq_af),family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_pH10~sequence+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_pH10~sequence+predicted_loop_type+index_sequence,family = Gamma,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```



```{r}
model=glm(deg_pH10~sequence+structure+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

```

AUTRES ESSAIS
```{r}
model = glm(deg_pH10  ~ sequence + struct_be + struct_af +
              index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_pH10  ~ sequence + seq_be + seq_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_be + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_pH10  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_af + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_af + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + seq_be + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_pH10  ~ sequence + struct_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_pH10  ~ sequence + struct_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

```


#### Optimisation

```{r}
formula_gamma_deg_pH10 =
  "deg_pH10 ~ sequence + predicted_loop_type + index_sequence"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_gamma_deg_pH10 = glm(formula = formula_gamma_deg_pH10,
                             family = Gamma, data = data_train)
```

```{r}
summary(model_gamma_deg_pH10)
```

```{r}
y_pred_gamma_train = predict.glm(model_gamma_deg_pH10,data_train,type="response")
error_gamma_train <- mean((y_pred_gamma_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_gamma_train)

y_pred_gamma_val=predict.glm(model_gamma_deg_pH10,data_val,type="response")
error_gamma_val <- mean((y_pred_gamma_val-data_val$deg_pH10)^2,na.rm=TRUE)
print(error_gamma_val)
```

```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_gamma")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_gamma_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_gamma_val)
rm(error_gamma_val, error_gamma_train, y_pred_gamma_train, y_pred_gamma_val)
```

##### StepAIC

```{r}
model_gamma_deg_pH10_aic = stepAIC(model_gamma_deg_pH10, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_gamma_deg_pH10_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_gamma_train_aic = predict.glm(model_gamma_deg_pH10_aic,data_train,type="response")
mean((y_pred_gamma_train_aic-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred_gamma_val_aic=predict.glm(model_gamma_deg_pH10_aic,data_val,type="response")
mean((y_pred_gamma_val_aic-data_val$deg_pH10)^2,na.rm=TRUE)
```

```{r}
rm(model_gamma_deg_pH10_aic, y_pred_gamma_train_aic, y_pred_gamma_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_gamma_deg_pH10 sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_gamma_deg_pH10)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_gamma_deg_pH10_val = glm(formula = formula_gamma_deg_pH10,
                                 family = Gamma, data = data_val)
data_val_pen = model.matrix(model_gamma_deg_pH10_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_gamma_deg_pH10_val)
```

On pénalise le model_gamma_deg_pH10 avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_pH10_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_gamma_deg_pH10_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_pH10_pen_ridge, "model_gamma_deg_pH10_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_pH10_pen_ridge <- readRDS("model_gamma_deg_pH10_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_gamma_train = predict(model_gamma_deg_pH10_pen_ridge, data_train_pen,
                                   type = "response", s = "lambda.min")
y_pred_ridge_gamma_val = predict(model_gamma_deg_pH10_pen_ridge, data_val_pen,
                                 type = "response", s = "lambda.min")
error_ridge_gamma_train <- mean((y_pred_ridge_gamma_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_ridge_gamma_train)
error_ridge_gamma_val <- mean((y_pred_ridge_gamma_val-data_val$deg_pH10)^2,na.rm=TRUE)
print(error_ridge_gamma_val)
```

On pénalise le model_gamma_deg_pH10 avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_pH10_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_gamma_deg_pH10_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_pH10_pen_lasso, "model_gamma_deg_pH10_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_pH10_pen_lasso <- readRDS("model_gamma_deg_pH10_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_gamma_train = predict(model_gamma_deg_pH10_pen_lasso, data_train_pen,
                                   type="response", s="lambda.min")
error_lasso_gamma_train <- mean((y_pred_lasso_gamma_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_lasso_gamma_train)

y_pred_lasso_gamma_val = predict(model_gamma_deg_pH10_pen_lasso, data_val_pen,
                                 type="response", s="lambda.min")
error_lasso_gamma_val <- mean((y_pred_lasso_gamma_val-data_val$deg_pH10)^2,na.rm=TRUE)
print(error_lasso_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_gamma_ridge", "model_gamma_lasso")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_ridge_gamma_train,
                             error_lasso_gamma_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_ridge_gamma_val, error_lasso_gamma_val)
rm(error_ridge_gamma_val, error_lasso_gamma_val,
   error_ridge_gamma_train, error_lasso_gamma_train,
   y_pred_ridge_gamma_train, y_pred_ridge_gamma_val,
   y_pred_lasso_gamma_train, y_pred_lasso_gamma_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_gamma_deg_pH10 =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_pH10,
#                  y_true_val = data_val$deg_pH10,
#                  family = Gamma, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_gamma_deg_pH10)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_gamma_deg_pH10_pen_enet = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_gamma_deg_pH10)
#coef(model_gamma_deg_pH10_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_pH10_pen_enet, "model_gamma_deg_pH10_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_pH10_pen_enet <- readRDS("model_gamma_deg_pH10_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_gamma_train = predict(model_gamma_deg_pH10_pen_enet, data_train_pen,
                                   type = "response", s = "lambda.min")
error_enet_gamma_train <- mean((y_pred_enet_gamma_train - data_train$deg_pH10)^2, na.rm=TRUE)
print(error_enet_gamma_train)

y_pred_enet_gamma_val = predict(model_gamma_deg_pH10_pen_enet, data_val_pen,
                                 type = "response", s = "lambda.min")
error_enet_gamma_val <- mean((y_pred_enet_gamma_val - data_val$deg_pH10)^2, na.rm=TRUE)
print(error_enet_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_gamma_enet")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_enet_gamma_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_enet_gamma_val)
rm(error_enet_gamma_train, error_enet_gamma_val,
   y_pred_enet_gamma_train, y_pred_enet_gamma_val)
```

```{r}
data.frame(models_deg_pH10, errors_deg_pH10_train, errors_deg_pH10_val)
```













### Inverse.gaussian

```{r}
model=glm(deg_pH10~sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_pH10~(sequence+seq_be),family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_pH10~index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_pH10~sequence+predicted_loop_type,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_pH10~sequence+structure,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_pH10)^2,na.rm=TRUE)
```




AUTRES ESSAIS
```{r}
#model = glm(deg_pH10  ~ sequence + struct_be + struct_af +
#              index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
#model = glm(deg_pH10  ~ sequence + seq_be + seq_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_be + struct_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_pH10  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = inverse.gaussian, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_af + struct_be +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_af + struct_af +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + seq_be + seq_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + struct_be + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
#model = glm(deg_pH10  ~ sequence + struct_af + predicted_loop_type +
#                         index_sequence + structure,
#            family = inverse.gaussian, data = data_train)
#
#y_pred=predict.glm(model,data_val,type="response")
#mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)
#
#y_pred=predict.glm(model,data_train,type="response")
#mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

#### Optimisation

```{r}
formula_inverse_gaussian_deg_pH10 =
  "deg_pH10 ~ sequence + predicted_loop_type"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_inverse_gaussian_deg_pH10 = glm(formula = formula_inverse_gaussian_deg_pH10,
                                        family = inverse.gaussian, data = data_train)
```

```{r}
summary(model_inverse_gaussian_deg_pH10)
```

```{r}
y_pred_inverse_gaussian_train = predict.glm(model_inverse_gaussian_deg_pH10,
                                            data_train, type = "response")
error_inverse_gaussian_train <-
  mean((y_pred_inverse_gaussian_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_inverse_gaussian_train)

y_pred_inverse_gaussian_val = predict.glm(model_inverse_gaussian_deg_pH10,
                                          data_val, type = "response")
error_inverse_gaussian_val <-
  mean((y_pred_inverse_gaussian_val - data_val$deg_pH10)^2,na.rm=TRUE)
print(error_inverse_gaussian_val)
```

```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_inverse_gaussian")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_inverse_gaussian_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_inverse_gaussian_val)
rm(error_inverse_gaussian_val, error_inverse_gaussian_train,
   y_pred_inverse_gaussian_train, y_pred_inverse_gaussian_val)
```

##### StepAIC

```{r}
model_inverse_gaussian_deg_pH10_aic =
  stepAIC(model_inverse_gaussian_deg_pH10, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_inverse_gaussian_deg_pH10_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_inverse_gaussian_train_aic = predict.glm(model_inverse_gaussian_deg_pH10_aic,data_train,type="response")
mean((y_pred_inverse_gaussian_train_aic-data_train$deg_pH10)^2,na.rm=TRUE)

y_pred_inverse_gaussian_val_aic=predict.glm(model_inverse_gaussian_deg_pH10_aic,data_val,type="response")
mean((y_pred_inverse_gaussian_val_aic-data_val$deg_pH10)^2,na.rm=TRUE)
```

```{r}
rm(model_inverse_gaussian_deg_pH10_aic, y_pred_inverse_gaussian_train_aic, y_pred_inverse_gaussian_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_inverse_gaussian_deg_pH10 sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_inverse_gaussian_deg_pH10)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_inverse_gaussian_deg_pH10_val = glm(formula = formula_inverse_gaussian_deg_pH10,
                                 family = inverse.gaussian, data = data_val)
data_val_pen = model.matrix(model_inverse_gaussian_deg_pH10_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_inverse_gaussian_deg_pH10_val)
```

On pénalise le model_inverse_gaussian_deg_pH10 avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_pH10_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_inverse_gaussian_deg_pH10_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_pH10_pen_ridge,
#        "model_inverse_gaussian_deg_pH10_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_pH10_pen_ridge <- 
  readRDS("model_inverse_gaussian_deg_pH10_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_inverse_gaussian_train = predict(model_inverse_gaussian_deg_pH10_pen_ridge,
                                              data_train_pen,
                                              type = "response", s = "lambda.min")
y_pred_ridge_inverse_gaussian_val = predict(model_inverse_gaussian_deg_pH10_pen_ridge,
                                            data_val_pen,
                                            type = "response", s = "lambda.min")

error_ridge_inverse_gaussian_train <-
  mean((y_pred_ridge_inverse_gaussian_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_train)

error_ridge_inverse_gaussian_val <-
  mean((y_pred_ridge_inverse_gaussian_val-data_val$deg_pH10)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_val)
```

On pénalise le model_inverse_gaussian_deg_pH10 avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_pH10_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_inverse_gaussian_deg_pH10_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_pH10_pen_lasso,
#        "model_inverse_gaussian_deg_pH10_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_pH10_pen_lasso <-
  readRDS("model_inverse_gaussian_deg_pH10_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_inverse_gaussian_train = predict(model_inverse_gaussian_deg_pH10_pen_lasso,
                                              data_train_pen,
                                              type="response", s="lambda.min")
error_lasso_inverse_gaussian_train <-
  mean((y_pred_lasso_inverse_gaussian_train-data_train$deg_pH10)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_train)

y_pred_lasso_inverse_gaussian_val = predict(model_inverse_gaussian_deg_pH10_pen_lasso,
                                            data_val_pen,
                                            type="response", s="lambda.min")
error_lasso_inverse_gaussian_val <-
  mean((y_pred_lasso_inverse_gaussian_val-data_val$deg_pH10)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_inverse_gaussian_ridge", "model_inverse_gaussian_lasso")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_ridge_inverse_gaussian_train,
                             error_lasso_inverse_gaussian_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val)
rm(error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val,
   error_ridge_inverse_gaussian_train, error_lasso_inverse_gaussian_train,
   y_pred_ridge_inverse_gaussian_train, y_pred_ridge_inverse_gaussian_val,
   y_pred_lasso_inverse_gaussian_train, y_pred_lasso_inverse_gaussian_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_inverse_gaussian_deg_pH10 =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_pH10,
#                  y_true_val = data_val$deg_pH10,
#                  family = inverse.gaussian, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_inverse_gaussian_deg_pH10)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_inverse_gaussian_deg_pH10_pen_enet = cv.glmnet(data_train_pen, data_train$deg_pH10,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_inverse_gaussian_deg_pH10)
#coef(model_inverse_gaussian_deg_pH10_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_pH10_pen_enet,
#        "model_inverse_gaussian_deg_pH10_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_pH10_pen_enet <-
  readRDS("model_inverse_gaussian_deg_pH10_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_inverse_gaussian_train = predict(model_inverse_gaussian_deg_pH10_pen_enet,
                                             data_train_pen,
                                             type = "response", s = "lambda.min")
error_enet_inverse_gaussian_train <-
  mean((y_pred_enet_inverse_gaussian_train - data_train$deg_pH10)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_train)

y_pred_enet_inverse_gaussian_val = predict(model_inverse_gaussian_deg_pH10_pen_enet,
                                           data_val_pen,
                                           type = "response", s = "lambda.min")
error_enet_inverse_gaussian_val <-
  mean((y_pred_enet_inverse_gaussian_val - data_val$deg_pH10)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_pH10 <- c(models_deg_pH10, "model_inverse_gaussian_enet")
errors_deg_pH10_train <- c(errors_deg_pH10_train, error_enet_inverse_gaussian_train)
errors_deg_pH10_val <- c(errors_deg_pH10_val, error_enet_inverse_gaussian_val)
rm(error_enet_inverse_gaussian_train, error_enet_inverse_gaussian_val,
   y_pred_enet_inverse_gaussian_train, y_pred_enet_inverse_gaussian_val)
```


### Récapitulatif des résultats

```{r}
models_deg_pH10_errors_df <- data.frame(models_deg_pH10,
                                          errors_deg_pH10_train,
                                          errors_deg_pH10_val)
```

```{r}
models_deg_pH10_errors_df
```

Le meilleur modèle (au sens de la minimisation du MSE sur la validation) est le `model_gamma`.
Ainsi, la pénalisation n'a pas permis d'améliorer les performances ici.
```{r}
models_deg_pH10_errors_df[which.min(errors_deg_pH10_val), ]
```














## deg_Mg_50C

### Loi Gamma

Ici on fera énormément de tests pour essayer de voir quels variables apportent des informations utiles.

```{r}
model=glm(deg_Mg_50C~sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_50C~(sequence+seq_be+seq_af),family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_50C~sequence+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_Mg_50C~sequence+predicted_loop_type+index_sequence,family = Gamma,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```



```{r}
model=glm(deg_Mg_50C~sequence+structure+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

```

AUTRES ESSAIS
```{r}
model = glm(deg_Mg_50C ~ sequence + struct_be + struct_af +
              index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_50C ~ sequence + seq_be + seq_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C ~ sequence + seq_be + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_Mg_50C  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C  ~ sequence + seq_af + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C  ~ sequence + seq_af + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C ~ sequence + seq_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C  ~ sequence + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C ~ sequence + seq_be + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C  ~ sequence + struct_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_Mg_50C  ~ sequence + struct_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

```

#### Optimisation

```{r}
formula_gamma_deg_Mg_50C =
  "deg_Mg_50C ~ sequence + predicted_loop_type + index_sequence"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_gamma_deg_Mg_50C = glm(formula = formula_gamma_deg_Mg_50C,
                             family = Gamma, data = data_train)
```

```{r}
summary(model_gamma_deg_Mg_50C)
```

```{r}
y_pred_gamma_train = predict.glm(model_gamma_deg_Mg_50C,data_train,type="response")
error_gamma_train <- mean((y_pred_gamma_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_gamma_train)

y_pred_gamma_val=predict.glm(model_gamma_deg_Mg_50C,data_val,type="response")
error_gamma_val <- mean((y_pred_gamma_val-data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_gamma_val)
```

```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_gamma")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_gamma_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_gamma_val)
rm(error_gamma_val, error_gamma_train, y_pred_gamma_train, y_pred_gamma_val)
```

##### StepAIC

```{r}
model_gamma_deg_Mg_50C_aic = stepAIC(model_gamma_deg_Mg_50C, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_gamma_deg_Mg_50C_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_gamma_train_aic = predict.glm(model_gamma_deg_Mg_50C_aic,data_train,type="response")
mean((y_pred_gamma_train_aic-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred_gamma_val_aic=predict.glm(model_gamma_deg_Mg_50C_aic,data_val,type="response")
mean((y_pred_gamma_val_aic-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
rm(model_gamma_deg_Mg_50C_aic, y_pred_gamma_train_aic, y_pred_gamma_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_gamma_deg_Mg_50C sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_gamma_deg_Mg_50C)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_gamma_deg_Mg_50C_val = glm(formula = formula_gamma_deg_Mg_50C,
                                 family = Gamma, data = data_val)
data_val_pen = model.matrix(model_gamma_deg_Mg_50C_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_gamma_deg_Mg_50C_val)
```

On pénalise le model_gamma_deg_Mg_50C avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_Mg_50C_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_gamma_deg_Mg_50C_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_50C_pen_ridge, "model_gamma_deg_Mg_50C_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_50C_pen_ridge <- readRDS("model_gamma_deg_Mg_50C_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_gamma_train = predict(model_gamma_deg_Mg_50C_pen_ridge, data_train_pen,
                                   type = "response", s = "lambda.min")
y_pred_ridge_gamma_val = predict(model_gamma_deg_Mg_50C_pen_ridge, data_val_pen,
                                 type = "response", s = "lambda.min")
error_ridge_gamma_train <- mean((y_pred_ridge_gamma_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_ridge_gamma_train)
error_ridge_gamma_val <- mean((y_pred_ridge_gamma_val-data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_ridge_gamma_val)
```

On pénalise le model_gamma_deg_Mg_50C avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_Mg_50C_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_gamma_deg_Mg_50C_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_50C_pen_lasso, "model_gamma_deg_Mg_50C_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_50C_pen_lasso <- readRDS("model_gamma_deg_Mg_50C_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_gamma_train = predict(model_gamma_deg_Mg_50C_pen_lasso, data_train_pen,
                                   type="response", s="lambda.min")
error_lasso_gamma_train <- mean((y_pred_lasso_gamma_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_lasso_gamma_train)

y_pred_lasso_gamma_val = predict(model_gamma_deg_Mg_50C_pen_lasso, data_val_pen,
                                 type="response", s="lambda.min")
error_lasso_gamma_val <- mean((y_pred_lasso_gamma_val-data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_lasso_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_gamma_ridge", "model_gamma_lasso")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_ridge_gamma_train,
                             error_lasso_gamma_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_ridge_gamma_val, error_lasso_gamma_val)
rm(error_ridge_gamma_val, error_lasso_gamma_val,
   error_ridge_gamma_train, error_lasso_gamma_train,
   y_pred_ridge_gamma_train, y_pred_ridge_gamma_val,
   y_pred_lasso_gamma_train, y_pred_lasso_gamma_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_gamma_deg_Mg_50C =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_Mg_50C,
#                  y_true_val = data_val$deg_Mg_50C,
#                  family = Gamma, cores = detectCores() - 2)
```

Au final, le meilleur modèle est celui avec $\alpha = 0.65$.
```{r}
#print(alpha_opti_enet_gamma_deg_Mg_50C)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_gamma_deg_Mg_50C_pen_enet = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_gamma_deg_Mg_50C)
#coef(model_gamma_deg_Mg_50C_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_Mg_50C_pen_enet, "model_gamma_deg_Mg_50C_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_Mg_50C_pen_enet <- readRDS("model_gamma_deg_Mg_50C_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_gamma_train = predict(model_gamma_deg_Mg_50C_pen_enet, data_train_pen,
                                   type = "response", s = "lambda.min")
error_enet_gamma_train <- mean((y_pred_enet_gamma_train - data_train$deg_Mg_50C)^2, na.rm=TRUE)
print(error_enet_gamma_train)

y_pred_enet_gamma_val = predict(model_gamma_deg_Mg_50C_pen_enet, data_val_pen,
                                 type = "response", s = "lambda.min")
error_enet_gamma_val <- mean((y_pred_enet_gamma_val - data_val$deg_Mg_50C)^2, na.rm=TRUE)
print(error_enet_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_gamma_enet")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_enet_gamma_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_enet_gamma_val)
rm(error_enet_gamma_train, error_enet_gamma_val,
   y_pred_enet_gamma_train, y_pred_enet_gamma_val)
```

```{r}
data.frame(models_deg_Mg_50C, errors_deg_Mg_50C_train, errors_deg_Mg_50C_val)
```















### Inverse.gaussian

```{r}
model=glm(deg_Mg_50C~sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_50C~(sequence+seq_be),family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_50C~sequence+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_Mg_50C~sequence+predicted_loop_type,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_Mg_50C~sequence+structure+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_50C)^2,na.rm=TRUE)
```




#### Optimisation

```{r}
formula_inverse_gaussian_deg_Mg_50C =
  "deg_Mg_50C ~ sequence + structure + index_sequence"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_inverse_gaussian_deg_Mg_50C = glm(formula = formula_inverse_gaussian_deg_Mg_50C,
                                        family = inverse.gaussian, data = data_train)
```

```{r}
summary(model_inverse_gaussian_deg_Mg_50C)
```

```{r}
y_pred_inverse_gaussian_train = predict.glm(model_inverse_gaussian_deg_Mg_50C,
                                            data_train, type = "response")
error_inverse_gaussian_train <-
  mean((y_pred_inverse_gaussian_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_inverse_gaussian_train)

y_pred_inverse_gaussian_val = predict.glm(model_inverse_gaussian_deg_Mg_50C,
                                          data_val, type = "response")
error_inverse_gaussian_val <-
  mean((y_pred_inverse_gaussian_val - data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_inverse_gaussian_val)
```

```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_inverse_gaussian")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_inverse_gaussian_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_inverse_gaussian_val)
rm(error_inverse_gaussian_val, error_inverse_gaussian_train,
   y_pred_inverse_gaussian_train, y_pred_inverse_gaussian_val)
```

##### StepAIC

```{r}
model_inverse_gaussian_deg_Mg_50C_aic =
  stepAIC(model_inverse_gaussian_deg_Mg_50C, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_inverse_gaussian_deg_Mg_50C_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_inverse_gaussian_train_aic = predict.glm(model_inverse_gaussian_deg_Mg_50C_aic,data_train,type="response")
mean((y_pred_inverse_gaussian_train_aic-data_train$deg_Mg_50C)^2,na.rm=TRUE)

y_pred_inverse_gaussian_val_aic=predict.glm(model_inverse_gaussian_deg_Mg_50C_aic,data_val,type="response")
mean((y_pred_inverse_gaussian_val_aic-data_val$deg_Mg_50C)^2,na.rm=TRUE)
```

```{r}
rm(model_inverse_gaussian_deg_Mg_50C_aic, y_pred_inverse_gaussian_train_aic, y_pred_inverse_gaussian_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_inverse_gaussian_deg_Mg_50C sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_inverse_gaussian_deg_Mg_50C)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_inverse_gaussian_deg_Mg_50C_val = glm(formula = formula_inverse_gaussian_deg_Mg_50C,
                                 family = inverse.gaussian, data = data_val)
data_val_pen = model.matrix(model_inverse_gaussian_deg_Mg_50C_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_inverse_gaussian_deg_Mg_50C_val)
```

On pénalise le model_inverse_gaussian_deg_Mg_50C avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_Mg_50C_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_inverse_gaussian_deg_Mg_50C_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_50C_pen_ridge,
#        "model_inverse_gaussian_deg_Mg_50C_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_50C_pen_ridge <- 
  readRDS("model_inverse_gaussian_deg_Mg_50C_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_50C_pen_ridge,
                                              data_train_pen,
                                              type = "response", s = "lambda.min")
y_pred_ridge_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_50C_pen_ridge,
                                            data_val_pen,
                                            type = "response", s = "lambda.min")

error_ridge_inverse_gaussian_train <-
  mean((y_pred_ridge_inverse_gaussian_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_train)

error_ridge_inverse_gaussian_val <-
  mean((y_pred_ridge_inverse_gaussian_val-data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_val)
```

On pénalise le model_inverse_gaussian_deg_Mg_50C avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_Mg_50C_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_inverse_gaussian_deg_Mg_50C_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_50C_pen_lasso,
#        "model_inverse_gaussian_deg_Mg_50C_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_50C_pen_lasso <-
  readRDS("model_inverse_gaussian_deg_Mg_50C_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_50C_pen_lasso,
                                              data_train_pen,
                                              type="response", s="lambda.min")
error_lasso_inverse_gaussian_train <-
  mean((y_pred_lasso_inverse_gaussian_train-data_train$deg_Mg_50C)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_train)

y_pred_lasso_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_50C_pen_lasso,
                                            data_val_pen,
                                            type="response", s="lambda.min")
error_lasso_inverse_gaussian_val <-
  mean((y_pred_lasso_inverse_gaussian_val-data_val$deg_Mg_50C)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_inverse_gaussian_ridge", "model_inverse_gaussian_lasso")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_ridge_inverse_gaussian_train,
                             error_lasso_inverse_gaussian_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val)
rm(error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val,
   error_ridge_inverse_gaussian_train, error_lasso_inverse_gaussian_train,
   y_pred_ridge_inverse_gaussian_train, y_pred_ridge_inverse_gaussian_val,
   y_pred_lasso_inverse_gaussian_train, y_pred_lasso_inverse_gaussian_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_inverse_gaussian_deg_Mg_50C =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_Mg_50C,
#                  y_true_val = data_val$deg_Mg_50C,
#                  family = inverse.gaussian, cores = detectCores() - 2)
```

Au final, le meilleur modèle est celui avec $\alpha = 0.55$.
```{r}
#print(alpha_opti_enet_inverse_gaussian_deg_Mg_50C)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_inverse_gaussian_deg_Mg_50C_pen_enet = cv.glmnet(data_train_pen, data_train$deg_Mg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_inverse_gaussian_deg_Mg_50C)
#coef(model_inverse_gaussian_deg_Mg_50C_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_Mg_50C_pen_enet,
#        "model_inverse_gaussian_deg_Mg_50C_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_Mg_50C_pen_enet <-
  readRDS("model_inverse_gaussian_deg_Mg_50C_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_inverse_gaussian_train = predict(model_inverse_gaussian_deg_Mg_50C_pen_enet,
                                             data_train_pen,
                                             type = "response", s = "lambda.min")
error_enet_inverse_gaussian_train <-
  mean((y_pred_enet_inverse_gaussian_train - data_train$deg_Mg_50C)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_train)

y_pred_enet_inverse_gaussian_val = predict(model_inverse_gaussian_deg_Mg_50C_pen_enet,
                                           data_val_pen,
                                           type = "response", s = "lambda.min")
error_enet_inverse_gaussian_val <-
  mean((y_pred_enet_inverse_gaussian_val - data_val$deg_Mg_50C)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_Mg_50C <- c(models_deg_Mg_50C, "model_inverse_gaussian_enet")
errors_deg_Mg_50C_train <- c(errors_deg_Mg_50C_train, error_enet_inverse_gaussian_train)
errors_deg_Mg_50C_val <- c(errors_deg_Mg_50C_val, error_enet_inverse_gaussian_val)
rm(error_enet_inverse_gaussian_train, error_enet_inverse_gaussian_val,
   y_pred_enet_inverse_gaussian_train, y_pred_enet_inverse_gaussian_val)
```


### Récapitulatif des résultats

```{r}
models_deg_Mg_50C_errors_df <- data.frame(models_deg_Mg_50C,
                                          errors_deg_Mg_50C_train,
                                          errors_deg_Mg_50C_val)
```

```{r}
models_deg_Mg_50C_errors_df
```

Le meilleur modèle (au sens de la minimisation du MSE sur la validation) est le `model_gamma`.
Ainsi, la pénalisation n'a pas permis d'améliorer les performances ici.
```{r}
models_deg_Mg_50C_errors_df[which.min(errors_deg_Mg_50C_val), ]
```








## deg_50C

### Loi Gamma

Ici on fera énormément de tests pour essayer de voir quels variables apportent des informations utiles.

```{r}
model=glm(deg_50C~sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_50C~(sequence+seq_be+seq_af),family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_50C~sequence+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_50C~sequence+predicted_loop_type+index_sequence,family = Gamma,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```



```{r}
model=glm(deg_50C~sequence+structure+index_sequence,family = Gamma,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

```

AUTRES ESSAIS
```{r}
model = glm(deg_50C ~ sequence + struct_be + struct_af +
              index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_50C ~ sequence + seq_be + seq_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C ~ sequence + seq_be + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```

```{r}
model = glm(deg_50C  ~ sequence + seq_be + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C  ~ sequence + seq_af + struct_be +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C  ~ sequence + seq_af + struct_af +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C ~ sequence + seq_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C  ~ sequence + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C ~ sequence + seq_be + seq_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C  ~ sequence + struct_be + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)
```


```{r}
model = glm(deg_50C  ~ sequence + struct_af + predicted_loop_type +
                         index_sequence + structure,
            family = Gamma, data = data_train)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_Mg_pH10)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_Mg_pH10)^2,na.rm=TRUE)

```



#### Optimisation

```{r}
formula_gamma_deg_50C =
  "deg_50C ~ sequence + predicted_loop_type + index_sequence"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_gamma_deg_50C = glm(formula = formula_gamma_deg_50C,
                             family = Gamma, data = data_train)
```

```{r}
summary(model_gamma_deg_50C)
```

```{r}
y_pred_gamma_train = predict.glm(model_gamma_deg_50C,data_train,type="response")
error_gamma_train <- mean((y_pred_gamma_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_gamma_train)

y_pred_gamma_val=predict.glm(model_gamma_deg_50C,data_val,type="response")
error_gamma_val <- mean((y_pred_gamma_val-data_val$deg_50C)^2,na.rm=TRUE)
print(error_gamma_val)
```

```{r}
models_deg_50C <- c(models_deg_50C, "model_gamma")
errors_deg_50C_train <- c(errors_deg_50C_train, error_gamma_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_gamma_val)
rm(error_gamma_val, error_gamma_train, y_pred_gamma_train, y_pred_gamma_val)
```

##### StepAIC

```{r}
model_gamma_deg_50C_aic = stepAIC(model_gamma_deg_50C, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_gamma_deg_50C_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_gamma_train_aic = predict.glm(model_gamma_deg_50C_aic,data_train,type="response")
mean((y_pred_gamma_train_aic-data_train$deg_50C)^2,na.rm=TRUE)

y_pred_gamma_val_aic=predict.glm(model_gamma_deg_50C_aic,data_val,type="response")
mean((y_pred_gamma_val_aic-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
rm(model_gamma_deg_50C_aic, y_pred_gamma_train_aic, y_pred_gamma_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_gamma_deg_50C sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_gamma_deg_50C)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_gamma_deg_50C_val = glm(formula = formula_gamma_deg_50C,
                                 family = Gamma, data = data_val)
data_val_pen = model.matrix(model_gamma_deg_50C_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_gamma_deg_50C_val)
```

On pénalise le model_gamma_deg_50C avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_50C_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_gamma_deg_50C_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_50C_pen_ridge, "model_gamma_deg_50C_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_50C_pen_ridge <- readRDS("model_gamma_deg_50C_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_gamma_train = predict(model_gamma_deg_50C_pen_ridge, data_train_pen,
                                   type = "response", s = "lambda.min")
y_pred_ridge_gamma_val = predict(model_gamma_deg_50C_pen_ridge, data_val_pen,
                                 type = "response", s = "lambda.min")
error_ridge_gamma_train <- mean((y_pred_ridge_gamma_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_ridge_gamma_train)
error_ridge_gamma_val <- mean((y_pred_ridge_gamma_val-data_val$deg_50C)^2,na.rm=TRUE)
print(error_ridge_gamma_val)
```

On pénalise le model_gamma_deg_50C avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_gamma_deg_50C_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_gamma_deg_50C_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_50C_pen_lasso, "model_gamma_deg_50C_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_50C_pen_lasso <- readRDS("model_gamma_deg_50C_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_gamma_train = predict(model_gamma_deg_50C_pen_lasso, data_train_pen,
                                   type="response", s="lambda.min")
error_lasso_gamma_train <- mean((y_pred_lasso_gamma_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_lasso_gamma_train)

y_pred_lasso_gamma_val = predict(model_gamma_deg_50C_pen_lasso, data_val_pen,
                                 type="response", s="lambda.min")
error_lasso_gamma_val <- mean((y_pred_lasso_gamma_val-data_val$deg_50C)^2,na.rm=TRUE)
print(error_lasso_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_50C <- c(models_deg_50C, "model_gamma_ridge", "model_gamma_lasso")
errors_deg_50C_train <- c(errors_deg_50C_train, error_ridge_gamma_train,
                             error_lasso_gamma_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_ridge_gamma_val, error_lasso_gamma_val)
rm(error_ridge_gamma_val, error_lasso_gamma_val,
   error_ridge_gamma_train, error_lasso_gamma_train,
   y_pred_ridge_gamma_train, y_pred_ridge_gamma_val,
   y_pred_lasso_gamma_train, y_pred_lasso_gamma_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_gamma_deg_50C =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_50C,
#                  y_true_val = data_val$deg_50C,
#                  family = Gamma, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_gamma_deg_50C)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_gamma_deg_50C_pen_enet = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = Gamma, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_gamma_deg_50C)
#coef(model_gamma_deg_50C_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_gamma_deg_50C_pen_enet, "model_gamma_deg_50C_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_gamma_deg_50C_pen_enet <- readRDS("model_gamma_deg_50C_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_gamma_train = predict(model_gamma_deg_50C_pen_enet, data_train_pen,
                                   type = "response", s = "lambda.min")
error_enet_gamma_train <- mean((y_pred_enet_gamma_train - data_train$deg_50C)^2, na.rm=TRUE)
print(error_enet_gamma_train)

y_pred_enet_gamma_val = predict(model_gamma_deg_50C_pen_enet, data_val_pen,
                                 type = "response", s = "lambda.min")
error_enet_gamma_val <- mean((y_pred_enet_gamma_val - data_val$deg_50C)^2, na.rm=TRUE)
print(error_enet_gamma_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_50C <- c(models_deg_50C, "model_gamma_enet")
errors_deg_50C_train <- c(errors_deg_50C_train, error_enet_gamma_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_enet_gamma_val)
rm(error_enet_gamma_train, error_enet_gamma_val,
   y_pred_enet_gamma_train, y_pred_enet_gamma_val)
```

```{r}
data.frame(models_deg_50C, errors_deg_50C_train, errors_deg_50C_val)
```













### Inverse.gaussian

```{r}
model=glm(deg_50C~sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_50C~(sequence+seq_be+seq_af),family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
model=glm(deg_50C~sequence+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_50C~sequence+predicted_loop_type+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)
y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)
```


```{r}
model=glm(deg_50C~sequence+structure+index_sequence,family = inverse.gaussian,data = data_train)
#summary(model)

y_pred=predict.glm(model,data_val,type="response")
mean((y_pred-data_val$deg_50C)^2,na.rm=TRUE)

y_pred=predict.glm(model,data_train,type="response")
mean((y_pred-data_train$deg_50C)^2,na.rm=TRUE)
```



#### Optimisation

```{r}
formula_inverse_gaussian_deg_50C =
  "deg_50C ~ sequence + predicted_loop_type + index_sequence"
```

On stocke le meilleur modèle pour la loi gamma.
```{r}
model_inverse_gaussian_deg_50C = glm(formula = formula_inverse_gaussian_deg_50C,
                                        family = inverse.gaussian, data = data_train)
```

```{r}
summary(model_inverse_gaussian_deg_50C)
```

```{r}
y_pred_inverse_gaussian_train = predict.glm(model_inverse_gaussian_deg_50C,
                                            data_train, type = "response")
error_inverse_gaussian_train <-
  mean((y_pred_inverse_gaussian_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_inverse_gaussian_train)

y_pred_inverse_gaussian_val = predict.glm(model_inverse_gaussian_deg_50C,
                                          data_val, type = "response")
error_inverse_gaussian_val <-
  mean((y_pred_inverse_gaussian_val - data_val$deg_50C)^2,na.rm=TRUE)
print(error_inverse_gaussian_val)
```

```{r}
models_deg_50C <- c(models_deg_50C, "model_inverse_gaussian")
errors_deg_50C_train <- c(errors_deg_50C_train, error_inverse_gaussian_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_inverse_gaussian_val)
rm(error_inverse_gaussian_val, error_inverse_gaussian_train,
   y_pred_inverse_gaussian_train, y_pred_inverse_gaussian_val)
```

##### StepAIC

```{r}
model_inverse_gaussian_deg_50C_aic =
  stepAIC(model_inverse_gaussian_deg_50C, trace = FALSE)
```

Pas de changement du modèle. Cela est sûrement dû au fait qu'on n'a pas assez de variables initialement.
```{r}
summary(model_inverse_gaussian_deg_50C_aic)
```

Pas de changement sur les performances de prédiction.
```{r}
y_pred_inverse_gaussian_train_aic = predict.glm(model_inverse_gaussian_deg_50C_aic,data_train,type="response")
mean((y_pred_inverse_gaussian_train_aic-data_train$deg_50C)^2,na.rm=TRUE)

y_pred_inverse_gaussian_val_aic=predict.glm(model_inverse_gaussian_deg_50C_aic,data_val,type="response")
mean((y_pred_inverse_gaussian_val_aic-data_val$deg_50C)^2,na.rm=TRUE)
```

```{r}
rm(model_inverse_gaussian_deg_50C_aic, y_pred_inverse_gaussian_train_aic, y_pred_inverse_gaussian_val_aic)
```

On va maintenant tester des pénalisations ridge, lasso et elastic-net.

##### Pénalisation ridge et lasso

On commence par calculer la matrice de design du model_inverse_gaussian_deg_50C sur le train et le test.
```{r}
# Matrice de design du train
data_train_pen = model.matrix(model_inverse_gaussian_deg_50C)
#head(data_train_pen)
```

```{r}
# Pour faire la matrice de design de la validation
model_inverse_gaussian_deg_50C_val = glm(formula = formula_inverse_gaussian_deg_50C,
                                 family = inverse.gaussian, data = data_val)
data_val_pen = model.matrix(model_inverse_gaussian_deg_50C_val)
#head(data_val_pen)

# On supprime le modèle sur le test
rm(model_inverse_gaussian_deg_50C_val)
```

On pénalise le model_inverse_gaussian_deg_50C avec une pénalité ridge.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_50C_pen_ridge = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 0)
## lambda.min est le lambda qui minimise l'erreur de cross validation
#coef(model_inverse_gaussian_deg_50C_pen_ridge, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_50C_pen_ridge,
#        "model_inverse_gaussian_deg_50C_pen_ridge.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_50C_pen_ridge <- 
  readRDS("model_inverse_gaussian_deg_50C_pen_ridge.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation ridge.
```{r}
y_pred_ridge_inverse_gaussian_train = predict(model_inverse_gaussian_deg_50C_pen_ridge,
                                              data_train_pen,
                                              type = "response", s = "lambda.min")
y_pred_ridge_inverse_gaussian_val = predict(model_inverse_gaussian_deg_50C_pen_ridge,
                                            data_val_pen,
                                            type = "response", s = "lambda.min")

error_ridge_inverse_gaussian_train <-
  mean((y_pred_ridge_inverse_gaussian_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_train)

error_ridge_inverse_gaussian_val <-
  mean((y_pred_ridge_inverse_gaussian_val-data_val$deg_50C)^2,na.rm=TRUE)
print(error_ridge_inverse_gaussian_val)
```

On pénalise le model_inverse_gaussian_deg_50C avec une pénalité lasso.
On choisira  dans la suite le $\lambda$ qui minimise l'erreur de cross validation avec `s = "lambda.min"`. On peut afficher les coefficients du modèle correspondant.
```{r}
#model_inverse_gaussian_deg_50C_pen_lasso = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE, alpha = 1)
#coef(model_inverse_gaussian_deg_50C_pen_lasso, s = "lambda.min")
```


Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_50C_pen_lasso,
#        "model_inverse_gaussian_deg_50C_pen_lasso.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_50C_pen_lasso <-
  readRDS("model_inverse_gaussian_deg_50C_pen_lasso.rds")
```


On calcule les prédictions et les erreurs pour le modèle avec pénalisation lasso.
```{r}
y_pred_lasso_inverse_gaussian_train = predict(model_inverse_gaussian_deg_50C_pen_lasso,
                                              data_train_pen,
                                              type="response", s="lambda.min")
error_lasso_inverse_gaussian_train <-
  mean((y_pred_lasso_inverse_gaussian_train-data_train$deg_50C)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_train)

y_pred_lasso_inverse_gaussian_val = predict(model_inverse_gaussian_deg_50C_pen_lasso,
                                            data_val_pen,
                                            type="response", s="lambda.min")
error_lasso_inverse_gaussian_val <-
  mean((y_pred_lasso_inverse_gaussian_val-data_val$deg_50C)^2,na.rm=TRUE)
print(error_lasso_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_50C <- c(models_deg_50C, "model_inverse_gaussian_ridge", "model_inverse_gaussian_lasso")
errors_deg_50C_train <- c(errors_deg_50C_train, error_ridge_inverse_gaussian_train,
                             error_lasso_inverse_gaussian_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val)
rm(error_ridge_inverse_gaussian_val, error_lasso_inverse_gaussian_val,
   error_ridge_inverse_gaussian_train, error_lasso_inverse_gaussian_train,
   y_pred_ridge_inverse_gaussian_train, y_pred_ridge_inverse_gaussian_val,
   y_pred_lasso_inverse_gaussian_train, y_pred_lasso_inverse_gaussian_val)
```

##### Pénalisation elastic-net

On choisit $\alpha$ qui minimise le MSE sur la validation.
```{r}
#alpha_opti_enet_inverse_gaussian_deg_50C =
#  alpha_opti_enet(data_train = data_train_pen, data_val = data_val_pen,
#                  y_true_train = data_train$deg_50C,
#                  y_true_val = data_val$deg_50C,
#                  family = inverse.gaussian, cores = detectCores() - 2)
```

Au final, le meilleur modèle est le lasso.
```{r}
#print(alpha_opti_enet_inverse_gaussian_deg_50C)
```

On construit le modèle avec le choix optimal pour $\alpha$.
```{r}
#model_inverse_gaussian_deg_50C_pen_enet = cv.glmnet(data_train_pen, data_train$deg_50C,
#                                             family = inverse.gaussian, type.measure = "mse",
#                                             standardize = FALSE,
#                                             alpha = alpha_opti_enet_inverse_gaussian_deg_50C)
#coef(model_inverse_gaussian_deg_50C_pen_enet, s = "lambda.min")
```

Sauvegarde du modèle.
```{r}
#saveRDS(model_inverse_gaussian_deg_50C_pen_enet,
#        "model_inverse_gaussian_deg_50C_pen_enet.rds")
```

Chargement du modèle.
```{r}
model_inverse_gaussian_deg_50C_pen_enet <-
  readRDS("model_inverse_gaussian_deg_50C_pen_enet.rds")
```

On calcule les prédictions et les erreurs pour le modèle avec pénalisation elastic-net.
```{r}
y_pred_enet_inverse_gaussian_train = predict(model_inverse_gaussian_deg_50C_pen_enet,
                                             data_train_pen,
                                             type = "response", s = "lambda.min")
error_enet_inverse_gaussian_train <-
  mean((y_pred_enet_inverse_gaussian_train - data_train$deg_50C)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_train)

y_pred_enet_inverse_gaussian_val = predict(model_inverse_gaussian_deg_50C_pen_enet,
                                           data_val_pen,
                                           type = "response", s = "lambda.min")
error_enet_inverse_gaussian_val <-
  mean((y_pred_enet_inverse_gaussian_val - data_val$deg_50C)^2, na.rm=TRUE)
print(error_enet_inverse_gaussian_val)
```

On stocke les résultats et on supprime les variables temporaires.
```{r}
models_deg_50C <- c(models_deg_50C, "model_inverse_gaussian_enet")
errors_deg_50C_train <- c(errors_deg_50C_train, error_enet_inverse_gaussian_train)
errors_deg_50C_val <- c(errors_deg_50C_val, error_enet_inverse_gaussian_val)
rm(error_enet_inverse_gaussian_train, error_enet_inverse_gaussian_val,
   y_pred_enet_inverse_gaussian_train, y_pred_enet_inverse_gaussian_val)
```


### Récapitulatif des résultats

```{r}
models_deg_50C_errors_df <- data.frame(models_deg_50C,
                                          errors_deg_50C_train,
                                          errors_deg_50C_val)
```

```{r}
models_deg_50C_errors_df
```

Le meilleur modèle (au sens de la minimisation du MSE sur la validation) est le `model_inverse_gaussian`.
Ainsi, la pénalisation n'a pas permis d'améliorer les performances ici.
```{r}
models_deg_50C_errors_df[which.min(errors_deg_50C_val), ]
```

Sauvegarde des résultats.
```{r}
#save.image(file = "GLM_optimization.Rdata")
```

Chargement des résultats.
```{r}
#load(file = "GLM_optimization.Rdata")
```
